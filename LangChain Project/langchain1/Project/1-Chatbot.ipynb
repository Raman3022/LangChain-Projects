{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cfcef7",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Building A Chatbot!\n",
    "\n",
    "here we are to design and implement an LLM_powered chatbot. This Chatbot will be able to have a conversation and remeber previous interactions \n",
    "\n",
    "Note : This chatbot will only use the language model to have the conversation there are several other related concepts that you may be looking for : \n",
    "\n",
    "1.Conversational RAG : Enable a chatbot experience over external source of data \n",
    "\n",
    "2.Agents : Build a chatbots that can take actions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efcc8c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## loading all the nevironment variables \n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44c9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000026F26E41490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000026F26E363F0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "324a4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello Raman Srivastava, it's nice to meet you! How can I help you today? üòÑ  \\n\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 16, 'total_tokens': 41, 'completion_time': 0.045454545, 'prompt_time': 0.002030455, 'queue_time': 0.24958269700000002, 'total_time': 0.047485}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-391ac6a1-0df2-47f4-9b3e-42e14720f3c1-0' usage_metadata={'input_tokens': 16, 'output_tokens': 25, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Invoke the model with a human message\n",
    "response = model.invoke([HumanMessage(content=\"hi, my name is Raman Srivastava\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e7b792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Raman Srivastava. üòä  I remember that from our first interaction.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 42, 'total_tokens': 61, 'completion_time': 0.034545455, 'prompt_time': 0.003034153, 'queue_time': 0.24736622899999997, 'total_time': 0.037579608}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-34ececc8-dd78-4244-a935-f0f665252e68-0', usage_metadata={'input_tokens': 42, 'output_tokens': 19, 'total_tokens': 61})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hi, my name is Raman Srivastava\"),\n",
    "        AIMessage(content=\"Hello Raman! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Hey What is my name?\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a122c",
   "metadata": {},
   "source": [
    "### Message history \n",
    "\n",
    "We can usea Message history class to wrap out model and make it statful; . This will keep record of inputs and outputs of the model, and store them in data store. Future interaction will htne load those messages and pass them into chain as part of the input.\n",
    "\n",
    "Lets see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7cf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id: str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d541301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76545176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure config is defined\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , my name is Raman Srivastava and i am ai engineer\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e813ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Raman Srivastava!  It's great to meet you.  \\n\\nBeing an AI engineer yourself, what are you working on these days? Anything exciting? üòä  \\n\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b56a93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Raman Srivastava, you told me earlier!  \\n\\nHow can I help you today, Raman? üòä  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 247, 'total_tokens': 274, 'completion_time': 0.049090909, 'prompt_time': 0.009400449, 'queue_time': 0.24840935699999997, 'total_time': 0.058491358}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f40fda7-cc16-4222-a863-bf2a2040b460-0', usage_metadata={'input_tokens': 247, 'output_tokens': 27, 'total_tokens': 274})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , what is my name\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79f7555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the config to a different session id and see the result\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , What is my name\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfa1c296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a friendly AI, and I don't have access to any information about you, including your name.  \\n\\nIf you'd like to tell me your name, I'd love to know! üòä  \\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8094e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , my namae is raman srivastava\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e41985fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Raman Srivastava! It's nice to meet you. üòä  \\n\\nHow can I help you today?  \\n\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0048bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , what is my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b12fe66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Raman Srivastava!  üòä \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145ab5b",
   "metadata": {},
   "source": [
    "Prompt  Templates :\n",
    "\n",
    "prompt template help to turn raw user information into a format that llm can work with in this case the raw user input is just a message ,which we are passing to the LLM . lets now make a that but more ccomplicated First, lets add in a system message with some custom instructions(but still taking message as input.). next we will add in more inputs besided the mmessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e694e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant answer all the question to best of your ability.\",\n",
    "        ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0598cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b01fae50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Raman, it's nice to meet you! üòä\\n\\nI'm happy to help answer any questions you have. Just ask away!  \\n\\nWhat can I do for you today? \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 30, 'total_tokens': 74, 'completion_time': 0.08, 'prompt_time': 0.005096349, 'queue_time': 0.906158664, 'total_time': 0.085096349}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e6711e0d-3495-4dcb-a34f-9c2d7f58ca6d-0', usage_metadata={'input_tokens': 30, 'output_tokens': 44, 'total_tokens': 74})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is raman\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee3a749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "044bccd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Raman, it's nice to meet you! üòä  \\n\\nWhat can I do for you today?\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config  = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi My name is Raman\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7213b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add more complexity \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant. answer al the questions to best of your ablity in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65ce2708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á Raman! \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•á ‡§∏‡§µ‡§æ‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡•á ‡§î‡§∞ ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§∞‡§π‡•Ç‡§Å‡§ó‡§æ‡•§  \\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"hi my name is Raman \")],\"language\": \"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79875f85",
   "metadata": {},
   "source": [
    "Lets now wrap this more complicated chain in messages history class.\n",
    "this time,because there were multiple keys input in the input, we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ca800",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1339a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f4083b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§®‡§Æ‡§∏‡•ç‡§§‡•á Raman, \n",
      "\n",
      "(Hello Raman,)\n",
      "\n",
      "‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à! \n",
      "\n",
      "(It's nice to meet you!) \n",
      "\n",
      "‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•ã‡§à ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å?\n",
      "\n",
      "(Can I help you with anything?) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi, my name is raman ?\")],\n",
    "        \"language\": \"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbbc6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi what is the my name\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed81ac58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ Raman ‡§π‡•à‡•§ \\n\\n(Your name is Raman.) \\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48d483",
   "metadata": {},
   "source": [
    "Managing te Conversation History \n",
    "\n",
    "One import concept to understand when building chatbots how to manage conversation history. If left unmanaged, the list will grow unbounded and protentially overflow the context window of the LLM. therefore it is important to add a step that limitts the size of the messages you are passing in. \n",
    "\n",
    "'trim_messages' = it helps to reduce the size of the messages you are passing in. uses partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "212417e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your age?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I am 23 years old', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your hobbies?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I love to play games and watch movies', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your favorite game?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My favorite game is chess', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your favorite movie?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My favorite movie is Avengers', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages \n",
    "trimmer = trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are a good assistant\"),\n",
    "    AIMessage(content=\"Hello, my name is raman?\"),\n",
    "    HumanMessage(content=\"What is your name?\"),\n",
    "    AIMessage(content=\"My name is raman\"),\n",
    "    HumanMessage(content=\"What is your age?\"),\n",
    "    AIMessage(content=\"I am 23 years old\"),\n",
    "    HumanMessage(content=\"What is your hobbies?\"),\n",
    "    AIMessage(content=\"I love to play games and watch movies\"),\n",
    "    HumanMessage(content=\"What is your favorite game?\"),\n",
    "    AIMessage(content=\"My favorite game is chess\"),\n",
    "    HumanMessage(content=\"What is your favorite movie?\"),\n",
    "    AIMessage(content=\"My favorite movie is Avengers\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b79cc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't experience emotions or have hobbies in the way humans do. I can process information and respond to your requests, which is kind of like a hobby for me!  \\n\\nDo you have any hobbies you'd like to tell me about?\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)  # preprocess messages\n",
    "    | prompt  # injects messages + language into template\n",
    "    | model   # calls the LLM\n",
    ")\n",
    "\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"What is your hobbies?\")],\n",
    "    \"language\": \"English\"\n",
    "})\n",
    "\n",
    "\n",
    "response.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40adca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Wrap this in the Message History\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afd36ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡•á ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç?  \\n\\n(I don't know your name. Can you tell me?) \\n\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": messages +  [HumanMessage(content=\"hi what is the my name\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
