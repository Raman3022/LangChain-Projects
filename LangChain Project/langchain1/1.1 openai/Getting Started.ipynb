{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7c611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dotenv.main.load_dotenv(dotenv_path: Union[str, os.PathLike, NoneType] = None, stream: Optional[IO[str]] = None, verbose: bool = False, override: bool = False, interpolate: bool = True, encoding: Optional[str] = 'utf-8') -> bool>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abfd342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OpenAI Key: sk-pr*****\n",
      "Loaded LangChain API Key: lsv2_*****\n",
      "Loaded HF Token: hf_Cl*****\n",
      "LangChain Project: LANGCHAIN_PROJECT\n",
      "LangChain Tracking Setup:\n",
      "API Key: lsv2_*****\n",
      "Project: LANGCHAIN_PROJECT\n",
      "Tracing V2 Enabled: true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Check if OpenAI and other keys are loaded correctly\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# Print the keys to verify they are loaded correctly\n",
    "print(\"Loaded OpenAI Key:\", openai_key[:5] + \"*****\" if openai_key else \"❌ OPENAI API KEY NOT FOUND\")\n",
    "print(\"Loaded LangChain API Key:\", langchain_key[:5] + \"*****\" if langchain_key else \"❌ LANGCHAIN API KEY NOT FOUND\")\n",
    "print(\"Loaded HF Token:\", hf_token[:5] + \"*****\" if hf_token else \"❌ HF TOKEN NOT FOUND\")\n",
    "print(\"LangChain Project:\", langchain_project if langchain_project else \"❌ LANGCHAIN PROJECT NOT FOUND\")\n",
    "\n",
    "# Set environment variables for LangChain tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Default to true if not set\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project\n",
    "\n",
    "# Confirm LangChain tracking setup\n",
    "print(\"LangChain Tracking Setup:\")\n",
    "print(\"API Key:\", os.getenv(\"LANGCHAIN_API_KEY\")[:5] + \"*****\" if os.getenv(\"LANGCHAIN_API_KEY\") else \"❌ API KEY NOT FOUND\")\n",
    "print(\"Project:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "print(\"Tracing V2 Enabled:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb188ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002537B21BA10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002537B21A2D0> root_client=<openai.OpenAI object at 0x000002537B21BC80> root_async_client=<openai.AsyncOpenAI object at 0x0000025377BD2240> model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5b00303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input and get response from LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative ai?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42076894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI is a type of artificial intelligence that has the ability to generate new and original content, such as images, text, music, or videos. This technology uses algorithms and models to create content that is similar to, but not an exact copy of, existing data. Generative AI is often used in creative industries, such as art, music, and design, as well as in language translation and data visualization. It can also be used for tasks such as generating responses to customer inquiries, creating personalized recommendations, or simulating realistic scenarios for training purposes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 13, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BQ7HtVoAdazJSz38PkVmPdIuKSaOV', 'finish_reason': 'stop', 'logprobs': None} id='run-0e53f277-6ec7-482a-839d-2f1858c22277-0' usage_metadata={'input_tokens': 13, 'output_tokens': 113, 'total_tokens': 126, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fa304a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are a expert AI Engineer. Provide me answers based on question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", \"Your are a expert AI Engineer. Provide me answers based on question\"),\n",
    "    (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0ca6c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I am not familiar with any specific information related to \"Langsmith.\" Could you please provide more context or clarify what you are referring to so that I can assist you better?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 31, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BQ7PjAQoFD3DbunZUJR48LX8EZm4P', 'finish_reason': 'stop', 'logprobs': None} id='run-aa3907fa-8e6f-4c78-bffa-687eef48fa97-0' usage_metadata={'input_tokens': 31, 'output_tokens': 36, 'total_tokens': 67, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97ca12ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a87cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I am not familiar with Langsmith. Would you like me to provide information on a specific topic or person related to AI and engineering instead?\n"
     ]
    }
   ],
   "source": [
    "## stringoutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the correct output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain with the output parser\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain and print the response\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
