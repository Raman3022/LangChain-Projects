{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## langchain Traceing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e483c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion -- from the website we need to scrape the data\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83eb113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1d87fb03b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/observability\")\n",
    "loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276a464d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nObservability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataAlerts in LangSmithConfiguring PagerDuty Integration for LangSmith AlertsConfiguring Webhook Notifications for LangSmith AlertsHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityOn this pageObservability Quick Start\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n4. Define your applicationâ€‹\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you\\'d like - just make sure\\nit has an LLM call!\\nApplication CodePythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\n5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\nNow when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.\\n\\n6. Trace entire applicationâ€‹\\nYou can also use the [traceable] decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});\\nNow if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\nNext stepsâ€‹\\nCongratulations! If you\\'ve made it this far, you\\'re well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e186520",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if it is huge documents we have to split it into chunks\n",
    "#load the data ---> docs---> divide into chunks --- > Vectors Embeddings ---> Vectors Store Db\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eee3071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='(Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataAlerts in LangSmithConfiguring PagerDuty Integration for LangSmith AlertsConfiguring Webhook Notifications for LangSmith AlertsHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityOn this pageObservability Quick Start'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='This tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content=\"4. Define your applicationâ€‹\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you'd like - just make sure\\nit has an LLM call!\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Application CodePythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='= new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Now when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='6. Trace entire applicationâ€‹\\nYou can also use the [traceable] decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Now if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content=\"Next stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "# converting into vector store\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95250c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d87a0607a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3ac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## query from thevector store db\n",
    "query = (\"angSmith makes this easy with the wrap_openai (Python) or wrapOpenAI\")\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666e259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b7cc9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context provided. If the answer is not in the context, say \"I don\\'t know\".\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001D87A6F1E80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D87A6C35F0>, root_client=<openai.OpenAI object at 0x000001D87A6AFFB0>, root_async_client=<openai.AsyncOpenAI object at 0x000001D87A6AED50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### retrieval chain, Document Chain \n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the question based on the context provided. If the answer is not in the context, say \"I don't know\".\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d3a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [8ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > chain:format_inputs] [11ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\",\n",
      "  \"context\": \"LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\",\n",
      "  \"context\": \"LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > prompt:ChatPromptTemplate] [3ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nAnswer the question based on the context provided. If the answer is not in the context, say \\\"I don't know\\\".\\n<context>\\nLangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\\n</context>\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > llm:ChatOpenAI] [1.07s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make it easy to use OpenAI, you can modify your code to use the wrap_openai (Python) or wrapOpenAI client instead of using the OpenAI client directly.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make it easy to use OpenAI, you can modify your code to use the wrap_openai (Python) or wrapOpenAI client instead of using the OpenAI client directly.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 38,\n",
      "                \"prompt_tokens\": 79,\n",
      "                \"total_tokens\": 117,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BQ9xk5XbxEMzmmgyw1bcPlfJLfnv6\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0d8d24fa-c49d-4bb9-bd2f-05418d1846b2-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 79,\n",
      "              \"output_tokens\": 38,\n",
      "              \"total_tokens\": 117,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 38,\n",
      "      \"prompt_tokens\": 79,\n",
      "      \"total_tokens\": 117,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BQ9xk5XbxEMzmmgyw1bcPlfJLfnv6\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"To make it easy to use OpenAI, you can modify your code to use the wrap_openai (Python) or wrapOpenAI client instead of using the OpenAI client directly.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:stuff_documents_chain] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"To make it easy to use OpenAI, you can modify your code to use the wrap_openai (Python) or wrapOpenAI client instead of using the OpenAI client directly.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make it easy to use OpenAI, you can modify your code to use the wrap_openai (Python) or wrapOpenAI client instead of using the OpenAI client directly.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\",\n",
    "    \"context\": [Document(page_content=\"LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI,All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly\")]\n",
    "\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d325e793",
   "metadata": {},
   "source": [
    "However we want the documents to frist come from the retriver we just set up that way we can use this retriver to dynamically select the most revelent documents and pass those in for given questuon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e902c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d87a0607a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input ---> Retriver ----> vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c383cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D87A0607A0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context provided. If the answer is not in the context, say \"I don\\'t know\".\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001D87A6F1E80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D87A6C35F0>, root_client=<openai.OpenAI object at 0x000001D87A6AFFB0>, root_async_client=<openai.AsyncOpenAI object at 0x000001D87A6AED50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ef82b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is wrap_openai and how does LangSmith make it easier to use?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] [2.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [2.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] [2.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\n\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\\\"<your-langsmith-api-key>\\\"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\\\"<your-openai-api-key>\\\"\\n\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\\\"Harrison worked at Kensho\\\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \\\"\\\"\\\"Answer the users question using only the provided information below:        {docs}\\\"\\\"\\\".format(docs=\\\"\\\\n\\\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},            {\\\"role\\\": \\\"user\\\", \\\"content\\\": question},        ],        model=\\\"gpt-4o-mini\\\",    )import { OpenAI } from \\\"openai\\\";import { wrapOpenAI } from \\\"langsmith/wrappers\\\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be\\n\\nNext stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\n\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\\\"<your-langsmith-api-key>\\\"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\\\"<your-openai-api-key>\\\"\\n\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\\\"Harrison worked at Kensho\\\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \\\"\\\"\\\"Answer the users question using only the provided information below:        {docs}\\\"\\\"\\\".format(docs=\\\"\\\\n\\\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},            {\\\"role\\\": \\\"user\\\", \\\"content\\\": question},        ],        model=\\\"gpt-4o-mini\\\",    )import { OpenAI } from \\\"openai\\\";import { wrapOpenAI } from \\\"langsmith/wrappers\\\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be\\n\\nNext stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\",\n",
      "  \"context\": \"5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\n\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\\\"<your-langsmith-api-key>\\\"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\\\"<your-openai-api-key>\\\"\\n\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\\\"Harrison worked at Kensho\\\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \\\"\\\"\\\"Answer the users question using only the provided information below:        {docs}\\\"\\\"\\\".format(docs=\\\"\\\\n\\\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},            {\\\"role\\\": \\\"user\\\", \\\"content\\\": question},        ],        model=\\\"gpt-4o-mini\\\",    )import { OpenAI } from \\\"openai\\\";import { wrapOpenAI } from \\\"langsmith/wrappers\\\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be\\n\\nNext stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is wrap_openai and how does LangSmith make it easier to use?\",\n",
      "  \"context\": \"5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\n\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\\\"<your-langsmith-api-key>\\\"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\\\"<your-openai-api-key>\\\"\\n\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\\\"Harrison worked at Kensho\\\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \\\"\\\"\\\"Answer the users question using only the provided information below:        {docs}\\\"\\\"\\\".format(docs=\\\"\\\\n\\\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},            {\\\"role\\\": \\\"user\\\", \\\"content\\\": question},        ],        model=\\\"gpt-4o-mini\\\",    )import { OpenAI } from \\\"openai\\\";import { wrapOpenAI } from \\\"langsmith/wrappers\\\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be\\n\\nNext stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nAnswer the question based on the context provided. If the answer is not in the context, say \\\"I don't know\\\".\\n<context>\\n5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\n\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\\\"<your-langsmith-api-key>\\\"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\\\"<your-openai-api-key>\\\"\\n\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\\\"Harrison worked at Kensho\\\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \\\"\\\"\\\"Answer the users question using only the provided information below:        {docs}\\\"\\\"\\\".format(docs=\\\"\\\\n\\\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},            {\\\"role\\\": \\\"user\\\", \\\"content\\\": question},        ],        model=\\\"gpt-4o-mini\\\",    )import { OpenAI } from \\\"openai\\\";import { wrapOpenAI } from \\\"langsmith/wrappers\\\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be\\n\\nNext stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n</context>\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] [834ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 17,\n",
      "                \"prompt_tokens\": 791,\n",
      "                \"total_tokens\": 808,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BQA7uRHChhVUbf6c2sr8l6XPeiiTo\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-427fd615-2c21-4766-9fb4-885cb7e1df9b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 791,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 808,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 17,\n",
      "      \"prompt_tokens\": 791,\n",
      "      \"total_tokens\": 808,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BQA7uRHChhVUbf6c2sr8l6XPeiiTo\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] [860ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] [863ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<answer>] [865ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:retrieval_chain] [3.23s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the response from the LLM\n",
    "\n",
    "## Define the retrieval chain if not already defined\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\":\"What is wrap_openai and how does LangSmith make it easier to use?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec04a404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is wrap_openai and how does LangSmith make it easier to use?',\n",
       " 'context': [Document(id='98a4bbec-c35e-4479-9f2b-3aef0a4f9d4e', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'),\n",
       "  Document(id='2cd0ac5a-5b8f-44f8-b4ce-7f020e8080a8', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='This tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"'),\n",
       "  Document(id='ead66908-b61c-4ea3-aaf0-371d5be0a68f', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be'),\n",
       "  Document(id='db17339c-b6da-4b1b-b615-dc551fcf11fa', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content=\"Next stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\")],\n",
       " 'answer': \"The tutorial is about tracing OpenAI calls using LangSmith's observability SDK.\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2bd78bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='98a4bbec-c35e-4479-9f2b-3aef0a4f9d4e', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='5. Trace OpenAI callsâ€‹\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'),\n",
       " Document(id='2cd0ac5a-5b8f-44f8-b4ce-7f020e8080a8', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='This tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependenciesâ€‹\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API keyâ€‹\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environmentâ€‹\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"'),\n",
       " Document(id='ead66908-b61c-4ea3-aaf0-371d5be0a68f', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be'),\n",
       " Document(id='db17339c-b6da-4b1b-b615-dc551fcf11fa', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content=\"Next stepsâ€‹\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response [\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5022629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
